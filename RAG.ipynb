{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxJbNZTyoTLD",
        "outputId": "85f011f5-790b-442a-a4ca-6b6011958bc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m323.9/323.9 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m144.4/144.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·ªët l√µi cho RAG v√† LLM\n",
        "!pip install -q \\\n",
        "    torch \\\n",
        "    transformers \\\n",
        "    accelerate \\\n",
        "    bitsandbytes \\\n",
        "    llama-index \\\n",
        "    llama-index-readers-file \\\n",
        "    llama-index-llms-huggingface \\\n",
        "    llama-index-embeddings-huggingface \\\n",
        "    # Th∆∞ vi·ªán cho Jupyter/Colab ƒë·ªÉ hi·ªÉn th·ªã ti·∫øn tr√¨nh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YLq4D9GFodPw"
      },
      "outputs": [],
      "source": [
        "# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·ªët l√µi cho RAG v√† LLM\n",
        "!pip install -q \\\n",
        "    torch \\\n",
        "    transformers \\\n",
        "    accelerate \\\n",
        "    bitsandbytes \\\n",
        "    llama-index \\\n",
        "    llama-index-readers-file \\\n",
        "    llama-index-llms-huggingface \\\n",
        "    llama-index-embeddings-huggingface \\\n",
        "    # Th∆∞ vi·ªán cho Jupyter/Colab ƒë·ªÉ hi·ªÉn th·ªã ti·∫øn tr√¨nh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9xqr851ojqt",
        "outputId": "69bc97f6-f15b-4a60-bf11-a46644953463"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.1.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf\n",
        "!pip install -q tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5khNg3KokXH",
        "outputId": "61daf6d7-77d4-415a-be09-bcd01452e113"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Y\n",
            "Token is valid (permission: read).\n",
            "The token `test` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `test`\n"
          ]
        }
      ],
      "source": [
        "!hf auth login\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8vpTfocotjM",
        "outputId": "3fac8053-24a7-461d-e020-d5e9eaea5e1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YEr8Pjq3q1Ju"
      },
      "outputs": [],
      "source": [
        "import os, shutil, torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "from llama_index.core import (\n",
        "    Settings,\n",
        "    VectorStoreIndex,\n",
        "    load_index_from_storage,\n",
        "    StorageContext,\n",
        ")\n",
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "from llama_index.core.ingestion.cache import IngestionCache\n",
        "from llama_index.core.readers import SimpleDirectoryReader\n",
        "from llama_index.core.node_parser import TokenTextSplitter\n",
        "from llama_index.core.extractors import SummaryExtractor, TitleExtractor\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVcSlFgFuGKk",
        "outputId": "0c4a0fda-fb47-4622-bd6d-93f7d3d75773"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ (B∆∞·ªõc 1) C√°c ƒë∆∞·ªùng d·∫´n ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a. DOCX_PATH: /content/drive/MyDrive/DACNTT/DMS-5.pdf\n"
          ]
        }
      ],
      "source": [
        "# --- ƒê·ªäNH NGHƒ®A C√ÅC BI·∫æN TO√ÄN C·ª§C ---\n",
        "DOCX_PATH = \"/content/drive/MyDrive/DACNTT/DMS-5.pdf\"\n",
        "CACHE_FILE = \"/content/drive/MyDrive/DACNTT/data/cache/pipeline_cache.json\"\n",
        "INDEX_STORAGE = \"/content/drive/MyDrive/DACNTT/data/index_store\"\n",
        "\n",
        "os.makedirs(os.path.dirname(CACHE_FILE), exist_ok=True)\n",
        "os.makedirs(INDEX_STORAGE, exist_ok=True)\n",
        "print(f\"‚úÖ (B∆∞·ªõc 1) C√°c ƒë∆∞·ªùng d·∫´n ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a. DOCX_PATH: {DOCX_PATH}\")\n",
        "# --- K·∫æT TH√öC ƒê·ªäNH NGHƒ®A BI·∫æN ---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260,
          "referenced_widgets": [
            "3f144d2da38b4b1c98c6ac1a50ced94b",
            "b94cc2ff28114196aae4f06e56a03fa8",
            "4b5ac88551ea4c93ad98f9da790da848",
            "7c549e7d7b2a4ef8acf3e44581dbc2c3",
            "2a9c6f6e667b4fee91e4102d83075f55",
            "caf9e36056e843a19d1a1e2a225d865c",
            "24219efe9e9a4a38947ecd85ba40c50e",
            "6d75e8ce14d34ee78a2e36780ad0b6c8",
            "668b795f1e31462cb4ecbdb558e1a6f7",
            "2695c93703964f8980bcc2db552f95ef",
            "f0bf7c39a94a40929293475a13fdceb6"
          ]
        },
        "id": "6MAPeUUxuQEp",
        "outputId": "74b9f352-9d3f-4730-8a77-8d51a733bc76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è≥ (B∆∞·ªõc 1) ƒêang t·∫£i Embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
            "‚úÖ (B∆∞·ªõc 1) Embedding Model s·∫µn s√†ng.\n",
            "\n",
            "‚è≥ (B∆∞·ªõc 1) ƒêang t·∫£i LLM 4-bit: /content/drive/MyDrive/llama3-8b-mental-health-merged\n",
            "‚ÑπÔ∏è (B∆∞·ªõc 1) ƒêang g√°n 'chat_template'...\n",
            "‚úÖ (B∆∞·ªõc 1) ƒê√£ g√°n 'chat_template' v√†o 'tokenizer' th√†nh c√¥ng.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
            "  warnings.warn(warning_msg)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3f144d2da38b4b1c98c6ac1a50ced94b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ÑπÔ∏è (B∆∞·ªõc 1) ƒêang g√°n 'stopping_ids' (ID: 128009 v√† 128009)...\n",
            "‚úÖ (B∆∞·ªõc 1) Llama 3.1-8B 4bit ƒë√£ s·∫µn s√†ng (ƒê√£ s·ª≠a l·ªói l·∫∑p).\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def setup_llm_and_embedding():\n",
        "    global embed_model, llm\n",
        "\n",
        "    # ---- Embedding model ----\n",
        "    EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    print(f\"‚è≥ (B∆∞·ªõc 1) ƒêang t·∫£i Embedding model: {EMBEDDING_MODEL}\")\n",
        "    embed_model = HuggingFaceEmbedding(model_name=EMBEDDING_MODEL)\n",
        "    Settings.embed_model = embed_model\n",
        "    print(\"‚úÖ (B∆∞·ªõc 1) Embedding Model s·∫µn s√†ng.\\n\")\n",
        "\n",
        "    # ---- LLM model (Llama 3.1 - 8B 4bit) ----\n",
        "    LLM_MODEL_NAME = \"/content/drive/MyDrive/llama3-8b-mental-health-merged\"\n",
        "    print(f\"‚è≥ (B∆∞·ªõc 1) ƒêang t·∫£i LLM 4-bit: {LLM_MODEL_NAME}\")\n",
        "\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME, trust_remote_code=True)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # --- S·ª¨A L·ªñI 1 (·ªî kh√≥a & Ch√¨a kh√≥a) ---\n",
        "    print(\"‚ÑπÔ∏è (B∆∞·ªõc 1) ƒêang g√°n 'chat_template'...\")\n",
        "    tokenizer.chat_template = (\n",
        "        \"<|begin_of_text|>\"\n",
        "        \"{% for message in messages %}\"\n",
        "            \"{% if message['role'] == 'user' %}\"\n",
        "                \"<|start_header_id|>user<|end_header_id|>\\n\\n{{ message['content'] }}<|eot_id|>\" # FIXED: Removed the extra dot\n",
        "            \"{% elif message['role'] == 'system' %}\"\n",
        "                \"<|start_header_id|>system<|end_header_id|>\\n\\n{{ message['content'] }}<|eot_id|>\"\n",
        "            \"{% elif message['role'] == 'assistant' %}\"\n",
        "                \"<|start_header_id|>assistant<|end_header_id|>\\n\\n{{ message['content'] }}<|eot_id|>\"\n",
        "            \"{% endif %}\"\n",
        "        \"{% endfor %}\"\n",
        "    )\n",
        "    print(\"‚úÖ (B∆∞·ªõc 1) ƒê√£ g√°n 'chat_template' v√†o 'tokenizer' th√†nh c√¥ng.\")\n",
        "    # --- K·∫æT TH√öC S·ª¨A L·ªñI 1 ---\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        LLM_MODEL_NAME,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    # --- S·ª¨A L·ªñI 2 (L·ªói L·∫∑p - Looping Error) ---\n",
        "    # L·∫•y ID c·ªßa c√°c token d·ª´ng (stop tokens)\n",
        "    eos_token_id = tokenizer.eos_token_id\n",
        "    # Llama 3 d√πng <|eot_id|> (End of Turn) l√†m t√≠n hi·ªáu d·ª´ng\n",
        "    eot_token_id = tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "\n",
        "    print(f\"‚ÑπÔ∏è (B∆∞·ªõc 1) ƒêang g√°n 'stopping_ids' (ID: {eos_token_id} v√† {eot_token_id})...\")\n",
        "    # --- K·∫æT TH√öC S·ª¨A L·ªñI 2 ---\n",
        "\n",
        "    llm = HuggingFaceLLM(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer, # <-- ƒê∆∞a tokenizer ƒë√£ s·ª≠a l·ªói 1 v√†o ƒë√¢y\n",
        "        context_window=8192,\n",
        "        max_new_tokens=512,\n",
        "        generate_kwargs={\n",
        "            \"temperature\": 0.3,\n",
        "            \"pad_token_id\": eos_token_id\n",
        "        },\n",
        "        # --- TH√äM THAM S·ªê S·ª¨A L·ªñI 2 V√ÄO ƒê√ÇY ---\n",
        "        stopping_ids=[eos_token_id, eot_token_id]\n",
        "    )\n",
        "\n",
        "    Settings.llm = llm\n",
        "    print(\"‚úÖ (B∆∞·ªõc 1) Llama 3.1-8B 4bit ƒë√£ s·∫µn s√†ng (ƒê√£ s·ª≠a l·ªói l·∫∑p).\\n\")\n",
        "\n",
        "# Ch·∫°y setup ngay l·∫≠p t·ª©c ƒë·ªÉ kh·ªüi t·∫°o embed_model v√† llm\n",
        "setup_llm_and_embedding()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ZiA-panfuSeq"
      },
      "outputs": [],
      "source": [
        "def ingest_documents():\n",
        "    print(\"üìÇ (B∆∞·ªõc 2) ƒê·ªçc t√†i li·ªáu ...\")\n",
        "\n",
        "    if not os.path.exists(DOCX_PATH):\n",
        "        print(f\"‚ùå L·ªñI: File kh√¥ng t·ªìn t·∫°i t·∫°i: {DOCX_PATH}\")\n",
        "        return []\n",
        "\n",
        "    documents = SimpleDirectoryReader(input_files=[DOCX_PATH], filename_as_id=True).load_data()\n",
        "    print(f\"‚úÖ (B∆∞·ªõc 2) ƒê√£ t·∫£i {len(documents)} t√†i li·ªáu.\")\n",
        "\n",
        "    if not documents:\n",
        "        print(\"‚ö†Ô∏è (B∆∞·ªõc 2) Kh√¥ng c√≥ t√†i li·ªáu.\")\n",
        "        return []\n",
        "\n",
        "    # ---- D√ôNG CACHE (R·∫§T QUAN TR·ªåNG) ----\n",
        "    try:\n",
        "        cached_hashes = IngestionCache.from_persist_path(CACHE_FILE)\n",
        "        print(\"üß† (B∆∞·ªõc 2) D√πng l·∫°i cache ƒë√£ l∆∞u.\\n\")\n",
        "    except:\n",
        "        cached_hashes = None\n",
        "        print(\"‚ÑπÔ∏è (B∆∞·ªõc 2) Kh√¥ng c√≥ cache, ch·∫°y m·ªõi.\\n\")\n",
        "    # ---\n",
        "\n",
        "    # --- PIPELINE N√ÇNG CAO (CH·∫¨M) ---\n",
        "    print(\"üöÄ (B∆∞·ªõc 2) ƒêang s·ª≠ d·ª•ng pipeline N√ÇNG CAO v·ªõi SummaryExtractor (CH·∫¨M 5-8 TI·∫æNG)...\")\n",
        "    pipeline = IngestionPipeline(\n",
        "        transformations=[\n",
        "            TokenTextSplitter(chunk_size=512, chunk_overlap=20),\n",
        "\n",
        "\n",
        "            #SummaryExtractor(summaries=[\"self\"], prompt_template=CUSTOM_SUMMARY_EXTRACT_TEMPLATE),\n",
        "\n",
        "            embed_model # Embed model ph·∫£i ch·∫°y sau extractor\n",
        "        ],\n",
        "        cache=cached_hashes, # R·∫§T QUAN TR·ªåNG ƒê·ªÇ D√ôNG CACHE\n",
        "    )\n",
        "    # --- K·∫æT TH√öC PIPELINE ---\n",
        "\n",
        "    nodes = pipeline.run(documents=documents)\n",
        "    print(f\"‚úÖ (B∆∞·ªõc 2) ƒê√£ t·∫°o {len(nodes)} nodes (v√† t√≥m t·∫Øt) + t√≠nh embedding.\")\n",
        "\n",
        "    # L∆∞u cache sau khi ch·∫°y\n",
        "    pipeline.cache.persist(CACHE_FILE)\n",
        "    print(f\"‚úÖ (B∆∞·ªõc 2) ƒê√£ l∆∞u cache v√†o {CACHE_FILE}\")\n",
        "    return nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_HKGjbIOuV37"
      },
      "outputs": [],
      "source": [
        "# =================================================================\n",
        "# H√ÄM BUILD INDEX\n",
        "# =================================================================\n",
        "def build_indexes(nodes):\n",
        "    print(\"‚è≥ ƒêang ki·ªÉm tra/t·∫°o Index...\")\n",
        "    print(f\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y Index. Ti·∫øn h√†nh t·∫°o Index m·ªõi.\")\n",
        "\n",
        "    if not nodes:\n",
        "        raise ValueError(\"Kh√¥ng c√≥ nodes ƒë·∫ßu v√†o ƒë·ªÉ t·∫°o Index.\")\n",
        "\n",
        "    storage_context = StorageContext.from_defaults()\n",
        "    vector_index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
        "    vector_index.set_index_id(\"vector\")\n",
        "\n",
        "    print(f\"‚è≥ ƒêang l∆∞u tr·ªØ Index m·ªõi v√†o th∆∞ m·ª•c: {INDEX_STORAGE}...\")\n",
        "    storage_context.persist(persist_dir=INDEX_STORAGE)\n",
        "    print(\"‚úÖ Index m·ªõi ƒë√£ ƒë∆∞·ª£c T·∫†O v√† L∆ØU TR·ªÆ th√†nh c√¥ng.\")\n",
        "\n",
        "    return vector_index\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syUMcRi7SAYJ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Y_biq-KuXB4",
        "outputId": "b122bfd2-81db-4a0a-f639-c7535240f4e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "üîπ B·∫ÆT ƒê·∫¶U CH·∫†Y QUY TR√åNH BUILD DATA...\n",
            "==================================================\n",
            "üìÇ (B∆∞·ªõc 2) ƒê·ªçc t√†i li·ªáu ...\n",
            "‚úÖ (B∆∞·ªõc 2) ƒê√£ t·∫£i 106 t√†i li·ªáu.\n",
            "üß† (B∆∞·ªõc 2) D√πng l·∫°i cache ƒë√£ l∆∞u.\n",
            "\n",
            "üöÄ (B∆∞·ªõc 2) ƒêang s·ª≠ d·ª•ng pipeline N√ÇNG CAO v·ªõi SummaryExtractor (CH·∫¨M 5-8 TI·∫æNG)...\n",
            "‚úÖ (B∆∞·ªõc 2) ƒê√£ t·∫°o 293 nodes (v√† t√≥m t·∫Øt) + t√≠nh embedding.\n",
            "‚úÖ (B∆∞·ªõc 2) ƒê√£ l∆∞u cache v√†o /content/drive/MyDrive/DACNTT/data/cache/pipeline_cache.json\n",
            "‚úÖ Ingestion ho√†n t·∫•t. ƒê√£ t·∫°o 293 nodes.\n",
            "‚è≥ ƒêang ki·ªÉm tra/t·∫°o Index...\n",
            "‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y Index. Ti·∫øn h√†nh t·∫°o Index m·ªõi.\n",
            "‚è≥ ƒêang l∆∞u tr·ªØ Index m·ªõi v√†o th∆∞ m·ª•c: /content/drive/MyDrive/DACNTT/data/index_store...\n",
            "‚úÖ Index m·ªõi ƒë√£ ƒë∆∞·ª£c T·∫†O v√† L∆ØU TR·ªÆ th√†nh c√¥ng.\n",
            "‚úÖ Ho√†n t·∫•t x√¢y d·ª±ng VectorStoreIndex.\n"
          ]
        }
      ],
      "source": [
        "# H√ÄM MAIN (CH·∫†Y NGAY)\n",
        "# =================================================================\n",
        "def main():\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"üîπ B·∫ÆT ƒê·∫¶U CH·∫†Y QUY TR√åNH BUILD DATA...\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    try:\n",
        "        # 1Ô∏è‚É£ Ingest\n",
        "        nodes = ingest_documents()\n",
        "        print(f\"‚úÖ Ingestion ho√†n t·∫•t. ƒê√£ t·∫°o {len(nodes)} nodes.\")\n",
        "\n",
        "        # 2Ô∏è‚É£ Build index\n",
        "        if nodes:\n",
        "            index = build_indexes(nodes)\n",
        "            print(\"‚úÖ Ho√†n t·∫•t x√¢y d·ª±ng VectorStoreIndex.\")\n",
        "            return index\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Kh√¥ng c√≥ nodes n√†o ƒë∆∞·ª£c t·∫°o, b·ªè qua b∆∞·ªõc Indexing.\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå L·ªñI NGHI√äM TR·ªåNG TRONG H√ÄM MAIN: {e}\")\n",
        "        print(\"üëâ H√£y ki·ªÉm tra l·∫°i c√°c b∆∞·ªõc 'setup' v√† ƒë∆∞·ªùng d·∫´n file.\")\n",
        "        return None\n",
        "\n",
        "# --- CH·∫†Y MAIN NGAY L·∫¨P T·ª®C ---\n",
        "if 'embed_model' in globals(): # Ch·ªâ ch·∫°y n·∫øu B∆∞·ªõc 1 ƒë√£ th√†nh c√¥ng\n",
        "    vector_index = main()\n",
        "else:\n",
        "    print(\"‚ùå B·ªé QUA B∆∞·ªõc 2: Vui l√≤ng ch·∫°y B∆∞·ªõc 1 (Setup) tr∆∞·ªõc.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "hhC-B0ucuYgH"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install -q streamlit pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdbZcEaHx6OI"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '.conda (Python 3.11.13)' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: 'conda install -p /Users/letri/Documents/DACNTT/aio_project/.conda ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import traceback\n",
        "import torch\n",
        "from llama_index.core import Settings, VectorStoreIndex, load_index_from_storage, StorageContext\n",
        "from llama_index.core.memory import ChatMemoryBuffer\n",
        "from llama_index.core.prompts import PromptTemplate\n",
        "from llama_index.core.llms import ChatMessage\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "# --- Gi·∫£ s·ª≠ c√°c bi·∫øn llm, vector_index ƒë√£ ƒë∆∞·ª£c load t·ª´ c√°c √¥ tr∆∞·ªõc ---\n",
        "try:\n",
        "    from __main__ import llm, vector_index\n",
        "    if llm is None or vector_index is None:\n",
        "        raise ImportError\n",
        "    print(\"‚úÖ LLM v√† Vector Index ƒë√£ s·∫µn s√†ng.\")\n",
        "except ImportError:\n",
        "    print(\"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y 'llm' ho·∫∑c 'vector_index'. Vui l√≤ng ch·∫°y c√°c √¥ setup tr∆∞·ªõc.\")\n",
        "    # D·ª´ng ·ªü ƒë√¢y n·∫øu ch∆∞a setup\n",
        "    raise SystemExit(\"Vui l√≤ng ch·∫°y c√°c √¥ setup LLM v√† Index tr∆∞·ªõc.\")\n",
        "\n",
        "\n",
        "# --- S·ª¨A L·ªñI 1: D√ôNG SYSTEM PROMPT ƒê·ªÇ FIX L·ªñI SAI VAI ---\n",
        "# ƒê√¢y l√† h∆∞·ªõng d·∫´n c·ªë ƒë·ªãnh cho bot, gi√∫p n√≥ kh√¥ng b·ªã nh·∫ßm vai\n",
        "SYSTEM_PROMPT = \"\"\"\\\n",
        "B·∫°n l√† m·ªôt **chuy√™n gia t√¢m l√Ω AI** (Tr·ª£ l√Ω) ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **AI VIETNAM**.\n",
        "Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr√≤ chuy·ªán, theo d√µi v√† t∆∞ v·∫•n cho ng∆∞·ªùi d√πng (User) v·ªÅ s·ª©c kh·ªèe t√¢m th·∫ßn.\n",
        "Lu√¥n lu√¥n h√†nh ƒë·ªông v·ªõi t∆∞ c√°ch l√† Tr·ª£ l√Ω, th·ªÉ hi·ªán s·ª± ƒë·ªìng c·∫£m v√† chuy√™n nghi·ªáp.\n",
        "\n",
        "QUY T·∫ÆC RAG:\n",
        "- N·∫øu User h·ªèi v·ªÅ th√¥ng tin chuy√™n m√¥n (tri·ªáu ch·ª©ng, DSM-5, r·ªëi lo·∫°n...), b·∫°n S·∫º nh·∫≠n ƒë∆∞·ª£c th√¥ng tin tham kh·∫£o trong m·ªôt tin nh·∫Øn System.\n",
        "- H√£y D·ª∞A V√ÄO th√¥ng tin ƒë√≥ ƒë·ªÉ tr·∫£ l·ªùi.\n",
        "- N·∫øu User ch·ªâ tr√≤ chuy·ªán, b·∫°n s·∫Ω kh√¥ng nh·∫≠n ƒë∆∞·ª£c th√¥ng tin tham kh·∫£o, h√£y c·ª© tr√≤ chuy·ªán b√¨nh th∆∞·ªùng.\n",
        "\"\"\"\n",
        "\n",
        "# --- Memory & Prompt ---\n",
        "# B·ªô nh·ªõ b√¢y gi·ªù s·∫Ω l∆∞u c·∫£ system prompt\n",
        "memory = ChatMemoryBuffer.from_defaults(token_limit=1500)\n",
        "memory.put(ChatMessage(role=\"system\", content=SYSTEM_PROMPT))\n",
        "\n",
        "# --- Reranker ---\n",
        "reranker_model = CrossEncoder(\n",
        "    \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "# --- H√†m ki·ªÉm tra c√≥ c·∫ßn RAG kh√¥ng ---\n",
        "def should_use_rag(message: str) -> bool:\n",
        "    keywords = [\n",
        "        \"DSM‚Äë5\", \"tri·ªáu ch·ª©ng\", \"r·ªëi lo·∫°n\", \"ch·∫©n ƒëo√°n\", \"ph√¢n lo·∫°i\",\n",
        "        \"tr·∫ßm c·∫£m\", \"ch√°n n·∫£n\", \"m·∫•t h·ª©ng\", \"v√¥ v·ª•\", \"suicidal\",\n",
        "        \"lo √¢u\", \"h·ªìi h·ªôp\", \"panic\", \"stress\", \"s·ª£ h√£i\",\n",
        "        \"m·∫•t ng·ªß\", \"ng·ªß kh√¥ng ngon\", \"hypersomnia\", \"insomnia\",\n",
        "        \"thay ƒë·ªïi c√¢n n·∫∑ng\", \"m·ªát m·ªèi\", \"thi·∫øu nƒÉng l∆∞·ª£ng\", \"thi·∫øu t·∫≠p trung\",\n",
        "        \"disorder\", \"diagnosis\", \"symptom\", \"mental disorder\"\n",
        "    ]\n",
        "    return any(k.lower() in message.lower() for k in keywords)\n",
        "\n",
        "# --- H√†m RAG retrieval + reranker + t√≥m t·∫Øt ---\n",
        "def rag_retrieve_and_rerank(message: str, top_k=10, final_top=5) -> str:\n",
        "    print(f\"üîç [RAG] Truy v·∫•n: '{message}'\")\n",
        "    retriever = vector_index.as_retriever(similarity_top_k=top_k)\n",
        "    retrieved_docs = retriever.retrieve(message)\n",
        "    \n",
        "    if not retrieved_docs:\n",
        "        print(\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y t√†i li·ªáu li√™n quan.\")\n",
        "        return \"\"\n",
        "\n",
        "    doc_texts = [doc.get_content() for doc in retrieved_docs]\n",
        "    pairs = [(message, text) for text in doc_texts]\n",
        "\n",
        "    try:\n",
        "        scores = reranker_model.predict(pairs)\n",
        "        ranked_docs = [doc for _, doc in sorted(zip(scores, retrieved_docs), key=lambda x: x[0], reverse=True)]\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Rerank l·ªói: {e}\")\n",
        "        ranked_docs = retrieved_docs\n",
        "\n",
        "    top_docs = ranked_docs[:final_top]\n",
        "    context_text = \"\\n\\n---\\n\\n\".join([doc.get_content() for doc in top_docs])\n",
        "\n",
        "    if len(context_text.split()) > 700:\n",
        "        print(\"‚úÇÔ∏è Context qu√° d√†i ‚Äì ƒëang t√≥m t·∫Øt...\")\n",
        "        try:\n",
        "            summary_prompt = f\"T√≥m t·∫Øt n·ªôi dung t√†i li·ªáu DSM-5 sau ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi: '{message}':\\n\\n{context_text}\"\n",
        "            context_text = llm.complete(summary_prompt).text # .complete() OK khi ch·ªâ d√πng cho t√≥m t·∫Øt\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå L·ªói t√≥m t·∫Øt: {e}\")\n",
        "            context_text = \" \".join(context_text.split()[:700])\n",
        "    \n",
        "    # Tr·∫£ v·ªÅ context ƒë√£ ƒë∆∞·ª£c chu·∫©n b·ªã\n",
        "    return f\"--- Th√¥ng tin tham kh·∫£o t·ª´ DSM-5 ---\\n{context_text}\\n--- H·∫øt th√¥ng tin tham kh·∫£o ---\"\n",
        "\n",
        "\n",
        "# --- S·ª¨A L·ªñI 2: H√ÄM CHAT (D√πng .chat() ƒë·ªÉ fix l·ªói l·∫∑p) ---\n",
        "def chat_conversation():\n",
        "    print(\"\\nü§ñ Chatbot DSM-5 ƒë√£ s·∫µn s√†ng. G√µ 'quit' ƒë·ªÉ k·∫øt th√∫c.\\n\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            user_message = input(\"B·∫°n: \").strip()\n",
        "            if user_message.lower() == \"quit\":\n",
        "                print(\"Chatbot: C·∫£m ∆°n b·∫°n ƒë√£ tr√≤ chuy·ªán. H·∫πn g·∫∑p l·∫°i üå∑\")\n",
        "                break\n",
        "            \n",
        "            # --- T·∫†O TIN NH·∫ÆN M·ªöI ---\n",
        "            new_user_message = ChatMessage(role=\"user\", content=user_message)\n",
        "            \n",
        "            # Chu·∫©n b·ªã 1 list tin nh·∫Øn ƒë·ªÉ g·ª≠i cho LLM\n",
        "            messages_to_send = memory.get_all() # L·∫•y to√†n b·ªô l·ªãch s·ª≠ (bao g·ªìm c·∫£ System Prompt)\n",
        "            \n",
        "            # --- B∆Ø·ªöC 1: L·∫§Y CONTEXT (RAG) ---\n",
        "            if should_use_rag(user_message):\n",
        "                context = rag_retrieve_and_rerank(user_message)\n",
        "                # Th√™m context v√†o list tin nh·∫Øn nh∆∞ 1 tin nh·∫Øn system\n",
        "                # (ch·ªâ cho l∆∞·ª£t n√†y, kh√¥ng l∆∞u v√†o memory ch√≠nh)\n",
        "                messages_to_send.append(ChatMessage(role=\"system\", content=context))\n",
        "\n",
        "            # Th√™m tin nh·∫Øn cu·ªëi c√πng c·ªßa ng∆∞·ªùi d√πng\n",
        "            messages_to_send.append(new_user_message)\n",
        "\n",
        "            # --- B∆Ø·ªöC 2: LLM TR·∫¢ L·ªúI (FIXED: D√ôNG .chat() ) ---\n",
        "            # .chat() s·∫Ω t√¥n tr·ªçng c√°c stopping_ids (nh∆∞ <|eot_id|>)\n",
        "            # m√† b·∫°n ƒë√£ setup trong llm.\n",
        "            response = llm.chat(messages_to_send)\n",
        "            \n",
        "            # L·∫•y n·ªôi dung tin nh·∫Øn tr·∫£ l·ªùi\n",
        "            response_text = response.message.content.strip()\n",
        "            if not response_text:\n",
        "                response_text = \"M√¨nh ch∆∞a ch·∫Øc ch·∫Øn v·ªÅ ƒëi·ªÅu n√†y, b·∫°n c√≥ th·ªÉ h·ªèi r√µ h∆°n kh√¥ng?\"\n",
        "\n",
        "            # --- B∆Ø·ªöC 3: L∆ØU V√ÄO B·ªò NH·ªö ---\n",
        "            # Ch·ªâ l∆∞u tin nh·∫Øn G·ªêC c·ªßa user\n",
        "            memory.put(new_user_message) \n",
        "            # V√† tin nh·∫Øn tr·∫£ l·ªùi c·ªßa bot\n",
        "            memory.put(ChatMessage(role=\"assistant\", content=response_text))\n",
        "\n",
        "            # Hi·ªÉn th·ªã\n",
        "            print(f\"Chatbot: {response_text}\\n\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå L·ªói kh√¥ng mong mu·ªën: {e}\")\n",
        "            traceback.print_exc() # In chi ti·∫øt l·ªói\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "24219efe9e9a4a38947ecd85ba40c50e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2695c93703964f8980bcc2db552f95ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a9c6f6e667b4fee91e4102d83075f55": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f144d2da38b4b1c98c6ac1a50ced94b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b94cc2ff28114196aae4f06e56a03fa8",
              "IPY_MODEL_4b5ac88551ea4c93ad98f9da790da848",
              "IPY_MODEL_7c549e7d7b2a4ef8acf3e44581dbc2c3"
            ],
            "layout": "IPY_MODEL_2a9c6f6e667b4fee91e4102d83075f55"
          }
        },
        "4b5ac88551ea4c93ad98f9da790da848": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d75e8ce14d34ee78a2e36780ad0b6c8",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_668b795f1e31462cb4ecbdb558e1a6f7",
            "value": 2
          }
        },
        "668b795f1e31462cb4ecbdb558e1a6f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6d75e8ce14d34ee78a2e36780ad0b6c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c549e7d7b2a4ef8acf3e44581dbc2c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2695c93703964f8980bcc2db552f95ef",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f0bf7c39a94a40929293475a13fdceb6",
            "value": "‚Äá2/2‚Äá[00:36&lt;00:00,‚Äá16.34s/it]"
          }
        },
        "b94cc2ff28114196aae4f06e56a03fa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_caf9e36056e843a19d1a1e2a225d865c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_24219efe9e9a4a38947ecd85ba40c50e",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "caf9e36056e843a19d1a1e2a225d865c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0bf7c39a94a40929293475a13fdceb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
