{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3f144d2da38b4b1c98c6ac1a50ced94b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b94cc2ff28114196aae4f06e56a03fa8",
              "IPY_MODEL_4b5ac88551ea4c93ad98f9da790da848",
              "IPY_MODEL_7c549e7d7b2a4ef8acf3e44581dbc2c3"
            ],
            "layout": "IPY_MODEL_2a9c6f6e667b4fee91e4102d83075f55"
          }
        },
        "b94cc2ff28114196aae4f06e56a03fa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_caf9e36056e843a19d1a1e2a225d865c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_24219efe9e9a4a38947ecd85ba40c50e",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "4b5ac88551ea4c93ad98f9da790da848": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d75e8ce14d34ee78a2e36780ad0b6c8",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_668b795f1e31462cb4ecbdb558e1a6f7",
            "value": 2
          }
        },
        "7c549e7d7b2a4ef8acf3e44581dbc2c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2695c93703964f8980bcc2db552f95ef",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f0bf7c39a94a40929293475a13fdceb6",
            "value": "‚Äá2/2‚Äá[00:36&lt;00:00,‚Äá16.34s/it]"
          }
        },
        "2a9c6f6e667b4fee91e4102d83075f55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "caf9e36056e843a19d1a1e2a225d865c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24219efe9e9a4a38947ecd85ba40c50e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d75e8ce14d34ee78a2e36780ad0b6c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "668b795f1e31462cb4ecbdb558e1a6f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2695c93703964f8980bcc2db552f95ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0bf7c39a94a40929293475a13fdceb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxJbNZTyoTLD",
        "outputId": "85f011f5-790b-442a-a4ca-6b6011958bc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m323.9/323.9 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m144.4/144.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·ªët l√µi cho RAG v√† LLM\n",
        "!pip install -q \\\n",
        "    torch \\\n",
        "    transformers \\\n",
        "    accelerate \\\n",
        "    bitsandbytes \\\n",
        "    llama-index \\\n",
        "    llama-index-readers-file \\\n",
        "    llama-index-llms-huggingface \\\n",
        "    llama-index-embeddings-huggingface \\\n",
        "    # Th∆∞ vi·ªán cho Jupyter/Colab ƒë·ªÉ hi·ªÉn th·ªã ti·∫øn tr√¨nh\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·ªët l√µi cho RAG v√† LLM\n",
        "!pip install -q \\\n",
        "    torch \\\n",
        "    transformers \\\n",
        "    accelerate \\\n",
        "    bitsandbytes \\\n",
        "    llama-index \\\n",
        "    llama-index-readers-file \\\n",
        "    llama-index-llms-huggingface \\\n",
        "    llama-index-embeddings-huggingface \\\n",
        "    # Th∆∞ vi·ªán cho Jupyter/Colab ƒë·ªÉ hi·ªÉn th·ªã ti·∫øn tr√¨nh\n"
      ],
      "metadata": {
        "id": "YLq4D9GFodPw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf\n",
        "!pip install -q tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9xqr851ojqt",
        "outputId": "69bc97f6-f15b-4a60-bf11-a46644953463"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hf auth login\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5khNg3KokXH",
        "outputId": "61daf6d7-77d4-415a-be09-bcd01452e113"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Y\n",
            "Token is valid (permission: read).\n",
            "The token `test` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `test`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8vpTfocotjM",
        "outputId": "3fac8053-24a7-461d-e020-d5e9eaea5e1f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "from llama_index.core import (\n",
        "    Settings,\n",
        "    VectorStoreIndex,\n",
        "    load_index_from_storage,\n",
        "    StorageContext,\n",
        ")\n",
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "from llama_index.core.ingestion.cache import IngestionCache\n",
        "from llama_index.core.readers import SimpleDirectoryReader\n",
        "from llama_index.core.node_parser import TokenTextSplitter\n",
        "from llama_index.core.extractors import SummaryExtractor, TitleExtractor\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n"
      ],
      "metadata": {
        "id": "YEr8Pjq3q1Ju"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ƒê·ªäNH NGHƒ®A C√ÅC BI·∫æN TO√ÄN C·ª§C ---\n",
        "DOCX_PATH = \"/content/drive/MyDrive/DACNTT/DMS-5.pdf\"\n",
        "CACHE_FILE = \"/content/drive/MyDrive/DACNTT/data/cache/pipeline_cache.json\"\n",
        "INDEX_STORAGE = \"/content/drive/MyDrive/DACNTT/data/index_store\"\n",
        "\n",
        "os.makedirs(os.path.dirname(CACHE_FILE), exist_ok=True)\n",
        "os.makedirs(INDEX_STORAGE, exist_ok=True)\n",
        "print(f\"‚úÖ (B∆∞·ªõc 1) C√°c ƒë∆∞·ªùng d·∫´n ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a. DOCX_PATH: {DOCX_PATH}\")\n",
        "# --- K·∫æT TH√öC ƒê·ªäNH NGHƒ®A BI·∫æN ---\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVcSlFgFuGKk",
        "outputId": "0c4a0fda-fb47-4622-bd6d-93f7d3d75773"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ (B∆∞·ªõc 1) C√°c ƒë∆∞·ªùng d·∫´n ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a. DOCX_PATH: /content/drive/MyDrive/DACNTT/DMS-5.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def setup_llm_and_embedding():\n",
        "    global embed_model, llm\n",
        "\n",
        "    # ---- Embedding model ----\n",
        "    EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    print(f\"‚è≥ (B∆∞·ªõc 1) ƒêang t·∫£i Embedding model: {EMBEDDING_MODEL}\")\n",
        "    embed_model = HuggingFaceEmbedding(model_name=EMBEDDING_MODEL)\n",
        "    Settings.embed_model = embed_model\n",
        "    print(\"‚úÖ (B∆∞·ªõc 1) Embedding Model s·∫µn s√†ng.\\n\")\n",
        "\n",
        "    # ---- LLM model (Llama 3.1 - 8B 4bit) ----\n",
        "    LLM_MODEL_NAME = \"/content/drive/MyDrive/llama3-8b-mental-health-merged\"\n",
        "    print(f\"‚è≥ (B∆∞·ªõc 1) ƒêang t·∫£i LLM 4-bit: {LLM_MODEL_NAME}\")\n",
        "\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME, trust_remote_code=True)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # --- S·ª¨A L·ªñI 1 (·ªî kh√≥a & Ch√¨a kh√≥a) ---\n",
        "    print(\"‚ÑπÔ∏è (B∆∞·ªõc 1) ƒêang g√°n 'chat_template'...\")\n",
        "    tokenizer.chat_template = (\n",
        "        \"<|begin_of_text|>\"\n",
        "        \"{% for message in messages %}\"\n",
        "            \"{% if message['role'] == 'user' %}\"\n",
        "                \"<|start_header_id|>user<|end_header_id|>\\n\\n{{ message['content'] }}<|eot_id|>\" # FIXED: Removed the extra dot\n",
        "            \"{% elif message['role'] == 'system' %}\"\n",
        "                \"<|start_header_id|>system<|end_header_id|>\\n\\n{{ message['content'] }}<|eot_id|>\"\n",
        "            \"{% elif message['role'] == 'assistant' %}\"\n",
        "                \"<|start_header_id|>assistant<|end_header_id|>\\n\\n{{ message['content'] }}<|eot_id|>\"\n",
        "            \"{% endif %}\"\n",
        "        \"{% endfor %}\"\n",
        "    )\n",
        "    print(\"‚úÖ (B∆∞·ªõc 1) ƒê√£ g√°n 'chat_template' v√†o 'tokenizer' th√†nh c√¥ng.\")\n",
        "    # --- K·∫æT TH√öC S·ª¨A L·ªñI 1 ---\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        LLM_MODEL_NAME,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    # --- S·ª¨A L·ªñI 2 (L·ªói L·∫∑p - Looping Error) ---\n",
        "    # L·∫•y ID c·ªßa c√°c token d·ª´ng (stop tokens)\n",
        "    eos_token_id = tokenizer.eos_token_id\n",
        "    # Llama 3 d√πng <|eot_id|> (End of Turn) l√†m t√≠n hi·ªáu d·ª´ng\n",
        "    eot_token_id = tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "\n",
        "    print(f\"‚ÑπÔ∏è (B∆∞·ªõc 1) ƒêang g√°n 'stopping_ids' (ID: {eos_token_id} v√† {eot_token_id})...\")\n",
        "    # --- K·∫æT TH√öC S·ª¨A L·ªñI 2 ---\n",
        "\n",
        "    llm = HuggingFaceLLM(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer, # <-- ƒê∆∞a tokenizer ƒë√£ s·ª≠a l·ªói 1 v√†o ƒë√¢y\n",
        "        context_window=8192,\n",
        "        max_new_tokens=512,\n",
        "        generate_kwargs={\n",
        "            \"temperature\": 0.3,\n",
        "            \"pad_token_id\": eos_token_id\n",
        "        },\n",
        "        # --- TH√äM THAM S·ªê S·ª¨A L·ªñI 2 V√ÄO ƒê√ÇY ---\n",
        "        stopping_ids=[eos_token_id, eot_token_id]\n",
        "    )\n",
        "\n",
        "    Settings.llm = llm\n",
        "    print(\"‚úÖ (B∆∞·ªõc 1) Llama 3.1-8B 4bit ƒë√£ s·∫µn s√†ng (ƒê√£ s·ª≠a l·ªói l·∫∑p).\\n\")\n",
        "\n",
        "# Ch·∫°y setup ngay l·∫≠p t·ª©c ƒë·ªÉ kh·ªüi t·∫°o embed_model v√† llm\n",
        "setup_llm_and_embedding()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260,
          "referenced_widgets": [
            "3f144d2da38b4b1c98c6ac1a50ced94b",
            "b94cc2ff28114196aae4f06e56a03fa8",
            "4b5ac88551ea4c93ad98f9da790da848",
            "7c549e7d7b2a4ef8acf3e44581dbc2c3",
            "2a9c6f6e667b4fee91e4102d83075f55",
            "caf9e36056e843a19d1a1e2a225d865c",
            "24219efe9e9a4a38947ecd85ba40c50e",
            "6d75e8ce14d34ee78a2e36780ad0b6c8",
            "668b795f1e31462cb4ecbdb558e1a6f7",
            "2695c93703964f8980bcc2db552f95ef",
            "f0bf7c39a94a40929293475a13fdceb6"
          ]
        },
        "id": "6MAPeUUxuQEp",
        "outputId": "74b9f352-9d3f-4730-8a77-8d51a733bc76"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ (B∆∞·ªõc 1) ƒêang t·∫£i Embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
            "‚úÖ (B∆∞·ªõc 1) Embedding Model s·∫µn s√†ng.\n",
            "\n",
            "‚è≥ (B∆∞·ªõc 1) ƒêang t·∫£i LLM 4-bit: /content/drive/MyDrive/llama3-8b-mental-health-merged\n",
            "‚ÑπÔ∏è (B∆∞·ªõc 1) ƒêang g√°n 'chat_template'...\n",
            "‚úÖ (B∆∞·ªõc 1) ƒê√£ g√°n 'chat_template' v√†o 'tokenizer' th√†nh c√¥ng.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
            "  warnings.warn(warning_msg)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3f144d2da38b4b1c98c6ac1a50ced94b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ÑπÔ∏è (B∆∞·ªõc 1) ƒêang g√°n 'stopping_ids' (ID: 128009 v√† 128009)...\n",
            "‚úÖ (B∆∞·ªõc 1) Llama 3.1-8B 4bit ƒë√£ s·∫µn s√†ng (ƒê√£ s·ª≠a l·ªói l·∫∑p).\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ingest_documents():\n",
        "    print(\"üìÇ (B∆∞·ªõc 2) ƒê·ªçc t√†i li·ªáu ...\")\n",
        "\n",
        "    if not os.path.exists(DOCX_PATH):\n",
        "        print(f\"‚ùå L·ªñI: File kh√¥ng t·ªìn t·∫°i t·∫°i: {DOCX_PATH}\")\n",
        "        return []\n",
        "\n",
        "    documents = SimpleDirectoryReader(input_files=[DOCX_PATH], filename_as_id=True).load_data()\n",
        "    print(f\"‚úÖ (B∆∞·ªõc 2) ƒê√£ t·∫£i {len(documents)} t√†i li·ªáu.\")\n",
        "\n",
        "    if not documents:\n",
        "        print(\"‚ö†Ô∏è (B∆∞·ªõc 2) Kh√¥ng c√≥ t√†i li·ªáu.\")\n",
        "        return []\n",
        "\n",
        "    # ---- D√ôNG CACHE (R·∫§T QUAN TR·ªåNG) ----\n",
        "    try:\n",
        "        cached_hashes = IngestionCache.from_persist_path(CACHE_FILE)\n",
        "        print(\"üß† (B∆∞·ªõc 2) D√πng l·∫°i cache ƒë√£ l∆∞u.\\n\")\n",
        "    except:\n",
        "        cached_hashes = None\n",
        "        print(\"‚ÑπÔ∏è (B∆∞·ªõc 2) Kh√¥ng c√≥ cache, ch·∫°y m·ªõi.\\n\")\n",
        "    # ---\n",
        "\n",
        "    # --- PIPELINE N√ÇNG CAO (CH·∫¨M) ---\n",
        "    print(\"üöÄ (B∆∞·ªõc 2) ƒêang s·ª≠ d·ª•ng pipeline N√ÇNG CAO v·ªõi SummaryExtractor (CH·∫¨M 5-8 TI·∫æNG)...\")\n",
        "    pipeline = IngestionPipeline(\n",
        "        transformations=[\n",
        "            TokenTextSplitter(chunk_size=512, chunk_overlap=20),\n",
        "\n",
        "\n",
        "            #SummaryExtractor(summaries=[\"self\"], prompt_template=CUSTOM_SUMMARY_EXTRACT_TEMPLATE),\n",
        "\n",
        "            embed_model # Embed model ph·∫£i ch·∫°y sau extractor\n",
        "        ],\n",
        "        cache=cached_hashes, # R·∫§T QUAN TR·ªåNG ƒê·ªÇ D√ôNG CACHE\n",
        "    )\n",
        "    # --- K·∫æT TH√öC PIPELINE ---\n",
        "\n",
        "    nodes = pipeline.run(documents=documents)\n",
        "    print(f\"‚úÖ (B∆∞·ªõc 2) ƒê√£ t·∫°o {len(nodes)} nodes (v√† t√≥m t·∫Øt) + t√≠nh embedding.\")\n",
        "\n",
        "    # L∆∞u cache sau khi ch·∫°y\n",
        "    pipeline.cache.persist(CACHE_FILE)\n",
        "    print(f\"‚úÖ (B∆∞·ªõc 2) ƒê√£ l∆∞u cache v√†o {CACHE_FILE}\")\n",
        "    return nodes"
      ],
      "metadata": {
        "id": "ZiA-panfuSeq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================\n",
        "# H√ÄM BUILD INDEX\n",
        "# =================================================================\n",
        "def build_indexes(nodes):\n",
        "    print(\"‚è≥ ƒêang ki·ªÉm tra/t·∫°o Index...\")\n",
        "    print(f\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y Index. Ti·∫øn h√†nh t·∫°o Index m·ªõi.\")\n",
        "\n",
        "    if not nodes:\n",
        "        raise ValueError(\"Kh√¥ng c√≥ nodes ƒë·∫ßu v√†o ƒë·ªÉ t·∫°o Index.\")\n",
        "\n",
        "    storage_context = StorageContext.from_defaults()\n",
        "    vector_index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
        "    vector_index.set_index_id(\"vector\")\n",
        "\n",
        "    print(f\"‚è≥ ƒêang l∆∞u tr·ªØ Index m·ªõi v√†o th∆∞ m·ª•c: {INDEX_STORAGE}...\")\n",
        "    storage_context.persist(persist_dir=INDEX_STORAGE)\n",
        "    print(\"‚úÖ Index m·ªõi ƒë√£ ƒë∆∞·ª£c T·∫†O v√† L∆ØU TR·ªÆ th√†nh c√¥ng.\")\n",
        "\n",
        "    return vector_index\n"
      ],
      "metadata": {
        "id": "_HKGjbIOuV37"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "syUMcRi7SAYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# H√ÄM MAIN (CH·∫†Y NGAY)\n",
        "# =================================================================\n",
        "def main():\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"üîπ B·∫ÆT ƒê·∫¶U CH·∫†Y QUY TR√åNH BUILD DATA...\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    try:\n",
        "        # 1Ô∏è‚É£ Ingest\n",
        "        nodes = ingest_documents()\n",
        "        print(f\"‚úÖ Ingestion ho√†n t·∫•t. ƒê√£ t·∫°o {len(nodes)} nodes.\")\n",
        "\n",
        "        # 2Ô∏è‚É£ Build index\n",
        "        if nodes:\n",
        "            index = build_indexes(nodes)\n",
        "            print(\"‚úÖ Ho√†n t·∫•t x√¢y d·ª±ng VectorStoreIndex.\")\n",
        "            return index\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Kh√¥ng c√≥ nodes n√†o ƒë∆∞·ª£c t·∫°o, b·ªè qua b∆∞·ªõc Indexing.\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå L·ªñI NGHI√äM TR·ªåNG TRONG H√ÄM MAIN: {e}\")\n",
        "        print(\"üëâ H√£y ki·ªÉm tra l·∫°i c√°c b∆∞·ªõc 'setup' v√† ƒë∆∞·ªùng d·∫´n file.\")\n",
        "        return None\n",
        "\n",
        "# --- CH·∫†Y MAIN NGAY L·∫¨P T·ª®C ---\n",
        "if 'embed_model' in globals(): # Ch·ªâ ch·∫°y n·∫øu B∆∞·ªõc 1 ƒë√£ th√†nh c√¥ng\n",
        "    vector_index = main()\n",
        "else:\n",
        "    print(\"‚ùå B·ªé QUA B∆∞·ªõc 2: Vui l√≤ng ch·∫°y B∆∞·ªõc 1 (Setup) tr∆∞·ªõc.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Y_biq-KuXB4",
        "outputId": "b122bfd2-81db-4a0a-f639-c7535240f4e9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "üîπ B·∫ÆT ƒê·∫¶U CH·∫†Y QUY TR√åNH BUILD DATA...\n",
            "==================================================\n",
            "üìÇ (B∆∞·ªõc 2) ƒê·ªçc t√†i li·ªáu ...\n",
            "‚úÖ (B∆∞·ªõc 2) ƒê√£ t·∫£i 106 t√†i li·ªáu.\n",
            "üß† (B∆∞·ªõc 2) D√πng l·∫°i cache ƒë√£ l∆∞u.\n",
            "\n",
            "üöÄ (B∆∞·ªõc 2) ƒêang s·ª≠ d·ª•ng pipeline N√ÇNG CAO v·ªõi SummaryExtractor (CH·∫¨M 5-8 TI·∫æNG)...\n",
            "‚úÖ (B∆∞·ªõc 2) ƒê√£ t·∫°o 293 nodes (v√† t√≥m t·∫Øt) + t√≠nh embedding.\n",
            "‚úÖ (B∆∞·ªõc 2) ƒê√£ l∆∞u cache v√†o /content/drive/MyDrive/DACNTT/data/cache/pipeline_cache.json\n",
            "‚úÖ Ingestion ho√†n t·∫•t. ƒê√£ t·∫°o 293 nodes.\n",
            "‚è≥ ƒêang ki·ªÉm tra/t·∫°o Index...\n",
            "‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y Index. Ti·∫øn h√†nh t·∫°o Index m·ªõi.\n",
            "‚è≥ ƒêang l∆∞u tr·ªØ Index m·ªõi v√†o th∆∞ m·ª•c: /content/drive/MyDrive/DACNTT/data/index_store...\n",
            "‚úÖ Index m·ªõi ƒë√£ ƒë∆∞·ª£c T·∫†O v√† L∆ØU TR·ªÆ th√†nh c√¥ng.\n",
            "‚úÖ Ho√†n t·∫•t x√¢y d·ª±ng VectorStoreIndex.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -q streamlit pyngrok"
      ],
      "metadata": {
        "id": "hhC-B0ucuYgH"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import traceback\n",
        "from llama_index.core import Settings, VectorStoreIndex\n",
        "from llama_index.core.memory import ChatMemoryBuffer\n",
        "from llama_index.core.prompts import PromptTemplate\n",
        "\n",
        "# ========================\n",
        "# 1Ô∏è‚É£ TEMPLATE DSM-5\n",
        "# ========================\n",
        "RAG_PROMPT_TEMPLATE = \"\"\"\\\n",
        "B·∫°n l√† m·ªôt **chuy√™n gia t√¢m l√Ω AI** ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi **AI VIETNAM**.\n",
        "Nhi·ªám v·ª• c·ªßa b·∫°n l√† **tr√≤ chuy·ªán, theo d√µi v√† t∆∞ v·∫•n** cho ng∆∞·ªùi d√πng v·ªÅ **s·ª©c kh·ªèe t√¢m th·∫ßn h√†ng ng√†y**.\n",
        "\n",
        "Th√¥ng tin ng∆∞·ªùi d√πng (n·∫øu c√≥):\n",
        "{user_info}\n",
        "\n",
        "---\n",
        "\n",
        "üéØ **H∆∞·ªõng d·∫´n h√†nh vi c·ªßa b·∫°n trong cu·ªôc tr√≤ chuy·ªán:**\n",
        "\n",
        "{context}\n",
        "\n",
        "Ng∆∞·ªùi d√πng h·ªèi: {user_message}\n",
        "Tr·∫£ l·ªùi:\n",
        "\"\"\"\n",
        "\n",
        "# ========================\n",
        "# 2Ô∏è‚É£ LOAD PIPELINE & INDEX\n",
        "# ========================\n",
        "try:\n",
        "    from __main__ import vector_index, llm, embed_model\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y bi·∫øn to√†n c·ª•c, ƒëang kh·ªüi t·∫°o l·∫°i...\")\n",
        "    from llama_index.core import load_index_from_storage, StorageContext\n",
        "    from pipeline import setup_llm_and_embedding, INDEX_STORAGE\n",
        "\n",
        "    llm, embed_model = setup_llm_and_embedding()\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=INDEX_STORAGE)\n",
        "    vector_index = load_index_from_storage(storage_context)\n",
        "\n",
        "Settings.llm = llm\n",
        "\n",
        "# ========================\n",
        "# 3Ô∏è‚É£ MEMORY CHAT\n",
        "# ========================\n",
        "memory = ChatMemoryBuffer.from_defaults(token_limit=1500)\n",
        "rag_prompt = PromptTemplate(RAG_PROMPT_TEMPLATE)\n",
        "\n",
        "# ========================\n",
        "# 4Ô∏è‚É£ CONDITIONAL RAG\n",
        "# ========================\n",
        "def should_use_rag(message: str) -> bool:\n",
        "    \"\"\"Trigger RAG only if the message likely needs DSM-5 data\"\"\"\n",
        "    keywords = [\"DSM-5\", \"tri·ªáu ch·ª©ng\", \"r·ªëi lo·∫°n\", \"ch·∫©n ƒëo√°n\", \"ph√¢n lo·∫°i\", \"tr·∫ßm c·∫£m\", \"lo √¢u\"]\n",
        "    return any(k.lower() in message.lower() for k in keywords)\n",
        "\n",
        "# ========================\n",
        "# 5Ô∏è‚É£ RE-RANKER b·∫±ng miniLM / cross-encoder\n",
        "# ========================\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "# Load cross-encoder model (miniLM)\n",
        "reranker_model = SentenceTransformer(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")  # t∆∞∆°ng ƒë·ªëi nh·∫π, t·ªëc ƒë·ªô nhanh\n",
        "\n",
        "def rag_retrieve_and_rerank(message: str, top_k=10, final_top=5) -> str:\n",
        "    \"\"\"Retrieve top-k docs, re-rank using cross-encoder, summarize top-n\"\"\"\n",
        "    # 1Ô∏è‚É£ Retrieval\n",
        "    retrieved_docs = vector_index.similarity_search(message, top_k=top_k)\n",
        "    if not retrieved_docs:\n",
        "        return \"\"\n",
        "\n",
        "    # 2Ô∏è‚É£ Re-rank using cross-encoder\n",
        "    doc_texts = [doc.text for doc in retrieved_docs]\n",
        "    scores = reranker_model.predict(list(zip([message]*len(doc_texts), doc_texts)))\n",
        "    ranked_docs = [doc for _, doc in sorted(zip(scores, retrieved_docs), key=lambda x: x[0], reverse=True)]\n",
        "\n",
        "    # 3Ô∏è‚É£ Take top-n documents\n",
        "    top_docs = ranked_docs[:final_top]\n",
        "\n",
        "    # 4Ô∏è‚É£ Summarize context to fit token limit\n",
        "    context_text = \" \".join([doc.text for doc in top_docs])\n",
        "    if len(context_text.split()) > 500:  # t·∫°m th·ªùi gi·ªõi h·∫°n 500 words\n",
        "        # d√πng LLM summarize context\n",
        "        context_text = llm.invoke(f\"T√≥m t·∫Øt ng·∫Øn g·ªçn vƒÉn b·∫£n sau, gi·ªØ th√¥ng tin DSM-5 ch√≠nh: {context_text}\")\n",
        "\n",
        "    return context_text\n",
        "\n",
        "# ========================\n",
        "# 6Ô∏è‚É£ MULTI-TURN CHAT + CONDITIONAL RAG\n",
        "# ========================\n",
        "def advanced_chat(message, history=[]):\n",
        "    try:\n",
        "        if not message.strip():\n",
        "            return \"‚ùå Vui l√≤ng nh·∫≠p c√¢u h·ªèi ho·∫∑c chia s·∫ª c·∫£m x√∫c.\"\n",
        "\n",
        "        # L·∫•y context n·∫øu c·∫ßn RAG\n",
        "        if should_use_rag(message):\n",
        "            context = rag_retrieve_and_rerank(message)\n",
        "            if not context:\n",
        "                context = \"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong DSM-5.\"\n",
        "        else:\n",
        "            context = \"LLM tr·∫£ l·ªùi tr·ª±c ti·∫øp, kh√¥ng c·∫ßn truy v·∫•n t√†i li·ªáu DSM-5.\"\n",
        "\n",
        "        # Compose prompt v·ªõi history + context\n",
        "        prompt = RAG_PROMPT_TEMPLATE.format(user_info=\"\", context=context, user_message=message)\n",
        "\n",
        "        # Truncate memory n·∫øu v∆∞·ª£t token limit\n",
        "        memory_text = memory.get_memory_as_text()\n",
        "        if len(memory_text.split()) > 1500:\n",
        "            summary = llm.invoke(f\"T√≥m t·∫Øt ng·∫Øn g·ªçn c√°c cu·ªôc tr√≤ chuy·ªán tr∆∞·ªõc ƒë√¢y:\\n{memory_text}\")\n",
        "            memory.clear()\n",
        "            memory.update_memory(summary)\n",
        "\n",
        "        # Generate response\n",
        "        response = llm.invoke(prompt)\n",
        "        clean_text = response.strip()\n",
        "        if not clean_text:\n",
        "            clean_text = \"‚ö†Ô∏è M√¨nh ch∆∞a ch·∫Øc ch·∫Øn, b·∫°n c√≥ th·ªÉ h·ªèi l·∫°i ho·∫∑c th·ª≠ c√¢u h·ªèi kh√°c.\"\n",
        "\n",
        "        # L∆∞u v√†o history\n",
        "        memory.update_memory(f\"user: {message}\\nassistant: {clean_text}\")\n",
        "\n",
        "        return clean_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(traceback.format_exc())\n",
        "        return f\"‚ùå L·ªói khi x·ª≠ l√Ω: {e}\"\n",
        "\n",
        "# ========================\n",
        "# 7Ô∏è‚É£ GRADIO INTERFACE\n",
        "# ========================\n",
        "title = \"üß† DSM-5 Chatbot N√¢ng Cao (Conditional RAG + Re-Ranker + Summarize)\"\n",
        "description = (\n",
        "    \"Tr·ª£ l√Ω AI chuy√™n v·ªÅ s·ª©c kh·ªèe t√¢m th·∫ßn. \"\n",
        "    \"LLM tr·∫£ l·ªùi tr·ª±c ti·∫øp ho·∫∑c truy v·∫•n DSM-5 khi c·∫ßn, Re-Rank + Summarize context.\"\n",
        ")\n",
        "\n",
        "iface = gr.ChatInterface(\n",
        "    fn=advanced_chat,\n",
        "    title=title,\n",
        "    description=description,\n",
        "    theme=\"soft\",\n",
        "    examples=[\n",
        "        [\"T√¥i c·∫£m th·∫•y r·∫•t m·ªát v√† kh√¥ng mu·ªën l√†m g√¨ c·∫£.\"],\n",
        "        [\"G·∫ßn ƒë√¢y t√¥i ng·ªß r·∫•t √≠t, b·∫°n nghƒ© t√¥i b·ªã g√¨ kh√¥ng?\"],\n",
        "        [\"T√¥i th·∫•y b·∫•t an v√† lo l·∫Øng v·ªÅ c√¥ng vi·ªác.\"],\n",
        "    ]\n",
        ")\n",
        "\n",
        "iface.launch(share=True, inbrowser=True)\n"
      ],
      "metadata": {
        "id": "WdbZcEaHx6OI"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4Ô∏è‚É£ H√†m x√°c ƒë·ªãnh khi n√†o c·∫ßn truy v·∫•n RAG\n",
        "# ============================================================\n",
        "def should_trigger_rag(message: str) -> bool:\n",
        "    \"\"\"Ph√°t hi·ªán xem c√¢u h·ªèi c√≥ c·∫ßn RAG kh√¥ng\"\"\"\n",
        "    trigger_phrases = [\n",
        "        \"ph√¢n t√≠ch\", \"tri·ªáu ch·ª©ng\", \"r·ªëi lo·∫°n\", \"dsm\", \"tr·∫ßm c·∫£m\",\n",
        "        \"lo √¢u\", \"stress\", \"t√¥i b·ªã\", \"h∆∞·ªõng d·∫´n\", \"chu·∫©n ƒëo√°n\"\n",
        "    ]\n",
        "    return any(kw in message.lower() for kw in trigger_phrases)\n",
        "\n",
        "# ============================================================\n",
        "# 5Ô∏è‚É£ H√†m re-rank t√†i li·ªáu (ƒë∆°n gi·∫£n)\n",
        "# ============================================================\n",
        "def rerank_nodes(query, nodes, top_n=3):\n",
        "    \"\"\"Gi·∫£ l·∫≠p re-ranker b·∫±ng ƒë·ªô d√†i vƒÉn b·∫£n\"\"\"\n",
        "    sorted_nodes = sorted(nodes, key=lambda x: -len(str(x.text)))\n",
        "    return sorted_nodes[:top_n]\n",
        "\n",
        "# ============================================================\n",
        "# 6Ô∏è‚É£ Format l·∫°i context ƒë∆∞a v√†o prompt\n",
        "# ============================================================\n",
        "def format_context(nodes):\n",
        "    blocks = [f\"[ƒêo·∫°n {i+1}]\\n{n.text.strip()}\" for i, n in enumerate(nodes)]\n",
        "    return \"\\n\\n\".join(blocks)\n",
        "\n",
        "# ============================================================\n",
        "# 7Ô∏è‚É£ H√†m chat ch√≠nh (RAG n√¢ng cao)\n",
        "# ============================================================\n",
        "def rag_chat(message, history=[]):\n",
        "    try:\n",
        "        if not message.strip():\n",
        "            return \"‚ùå Vui l√≤ng nh·∫≠p c√¢u h·ªèi ho·∫∑c chia s·∫ª c·∫£m x√∫c.\"\n",
        "\n",
        "        # (1) L∆∞u v√†o b·ªô nh·ªõ h·ªôi tho·∫°i\n",
        "        memory.put_message(\"user\", message)\n",
        "\n",
        "        # (2) Ki·ªÉm tra c√≥ c·∫ßn RAG hay kh√¥ng\n",
        "        use_rag = should_trigger_rag(message)\n",
        "\n",
        "        if not use_rag:\n",
        "            # N·∫øu kh√¥ng c·∫ßn RAG ‚Üí LLM tr·∫£ l·ªùi t·ª± nhi√™n\n",
        "            response = llm.complete(\n",
        "                f\"Ng∆∞·ªùi d√πng n√≥i: {message}\\n\"\n",
        "                \"‚Üí H√£y ph·∫£n h·ªìi nh·∫π nh√†ng, ƒë·ªìng c·∫£m, kh√¥ng d√πng thu·∫≠t ng·ªØ chuy√™n m√¥n.\"\n",
        "            )\n",
        "            return response.text.strip()\n",
        "\n",
        "        # (3) T·∫°o retriever v√† truy v·∫•n vector index\n",
        "        retriever = vector_index.as_retriever(similarity_top_k=10)\n",
        "        query_engine = RetrieverQueryEngine.from_args(retriever=retriever)\n",
        "        retrieved_nodes = query_engine.retrieve(message)\n",
        "\n",
        "        if not retrieved_nodes:\n",
        "            return \"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p t·ª´ DSM-5.\"\n",
        "\n",
        "        # (4) Re-rank t√†i li·ªáu\n",
        "        top_nodes = rerank_nodes(message, retrieved_nodes, top_n=3)\n",
        "\n",
        "        # (5) T·∫°o context\n",
        "        formatted_context = format_context(top_nodes)\n",
        "\n",
        "        # (6) Gh√©p prompt m·ªõi\n",
        "        user_info = memory.get_all()\n",
        "        prompt = f\"\"\"\n",
        "B·∫°n l√† chuy√™n gia t√¢m l√Ω AI h·ªó tr·ª£ ng∆∞·ªùi d√πng d·ª±a tr√™n ti√™u chu·∫©n DSM-5.\n",
        "\n",
        "Th√¥ng tin ng∆∞·ªùi d√πng (ƒë∆∞·ª£c nh·ªõ t·ª´ h·ªôi tho·∫°i):\n",
        "{user_info}\n",
        "\n",
        "Th√¥ng tin tr√≠ch xu·∫•t t·ª´ DSM-5:\n",
        "{formatted_context}\n",
        "\n",
        "C√¢u h·ªèi ng∆∞·ªùi d√πng:\n",
        "{message}\n",
        "\n",
        "‚Üí H√£y tr·∫£ l·ªùi ng·∫Øn g·ªçn, c√≥ d·∫´n ch·ª©ng t·ª´ DSM-5 khi c·∫ßn,\n",
        "v√† ƒë∆∞a ra l·ªùi khuy√™n nh·∫π nh√†ng, th√¢n thi·ªán, tr√°nh ch·∫©n ƒëo√°n y khoa ch√≠nh th·ª©c.\n",
        "        \"\"\"\n",
        "\n",
        "        # (7) Sinh ph·∫£n h·ªìi t·ª´ LLM\n",
        "        response = llm.complete(prompt)\n",
        "        answer = response.text.strip()\n",
        "\n",
        "        # (8) Ghi nh·ªõ ph·∫£n h·ªìi\n",
        "        memory.put_message(\"assistant\", answer)\n",
        "        return answer\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print(traceback.format_exc())\n",
        "        return f\"‚ùå L·ªói khi x·ª≠ l√Ω: {e}\"\n",
        "\n",
        "# ============================================================\n",
        "# 8Ô∏è‚É£ Giao di·ªán Gradio\n",
        "# ============================================================\n",
        "chatbot_ui = gr.ChatInterface(\n",
        "    fn=rag_chat,\n",
        "    title=\"üß© DSM-5 AI Psych Assistant (RAG+Memory)\",\n",
        "    description=\"Tr·ª£ l√Ω t√¢m l√Ω d·ª±a tr√™n DSM-5. C√≥ th·ªÉ ƒë·ªìng c·∫£m ho·∫∑c ph√¢n t√≠ch tri·ªáu ch·ª©ng theo y√™u c·∫ßu.\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "cmOAXIMja1rN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}