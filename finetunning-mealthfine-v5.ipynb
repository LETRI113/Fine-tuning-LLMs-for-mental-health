{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!pip install -U bitsandbytes accelerate transformers trl peft","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ‚úÖ 1. ƒêƒÉng nh·∫≠p Hugging Face\nfrom huggingface_hub import login\nlogin(new_session=False)  #","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T09:16:30.339521Z","iopub.execute_input":"2025-11-05T09:16:30.339798Z","iopub.status.idle":"2025-11-05T09:16:30.526715Z","shell.execute_reply.started":"2025-11-05T09:16:30.339777Z","shell.execute_reply":"2025-11-05T09:16:30.525940Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig, get_peft_model\nfrom trl import SFTTrainer\nimport torch, math\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T09:16:32.732663Z","iopub.execute_input":"2025-11-05T09:16:32.733416Z","iopub.status.idle":"2025-11-05T09:17:01.468009Z","shell.execute_reply.started":"2025-11-05T09:16:32.733390Z","shell.execute_reply":"2025-11-05T09:17:01.467212Z"}},"outputs":[{"name":"stderr","text":"2025-11-05 09:16:43.704007: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762334203.912356     110 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762334203.975873     110 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from datasets import load_dataset\n\n# 1Ô∏è‚É£ Load dataset\ndataset = load_dataset(\"hoangcung165/Metal-Health-Data\")\n\n# 2Ô∏è‚É£ (T√πy ch·ªçn) L·∫•y 1000 m·∫´u ƒë·∫ßu ƒë·ªÉ test\ntrain_subset = dataset[\"train\"].shuffle(seed=42).select(range(5000))\n\n# 3Ô∏è‚É£ H√†m ti·ªÅn x·ª≠ l√Ω theo format c·ªßa Llama 3 Instruct\ndef preprocess_function(example):\n    system_prompt = \"You are a helpful and empathetic AI assistant supporting users in mental health conversations.\"\n\n    # B·∫Øt ƒë·∫ßu b·∫±ng system prompt\n    formatted_text = (\n        \"<|begin_of_text|>\"\n        \"<|start_header_id|>system<|end_header_id|>\\n\"\n        f\"{system_prompt}<|eot_id|>\"\n    )\n\n    # L·∫∑p qua danh s√°ch h·ªôi tho·∫°i trong m·ªói example\n    for turn in example[\"conversations\"]:\n        role = \"user\" if turn[\"from\"] == \"human\" else \"assistant\"\n        content = turn[\"value\"].strip()\n        formatted_text += (\n            f\"<|start_header_id|>{role}<|end_header_id|>\\n\"\n            f\"{content}<|eot_id|>\"\n        )\n\n    # K·∫øt th√∫c b·∫±ng token cu·ªëi h·ªôi tho·∫°i\n    formatted_text += \"<|end_of_text|>\"\n\n    return {\"text\": formatted_text}\n\n# 4Ô∏è‚É£ X·ª≠ l√Ω dataset\nprint(\"üîß ƒêang x·ª≠ l√Ω dataset...\")\nprocessed_ds = train_subset.map(preprocess_function, remove_columns=[\"conversations\"])\n\n# 5Ô∏è‚É£ Hi·ªÉn th·ªã m·∫´u sau khi x·ª≠ l√Ω\nprint(\"‚úÖ M·∫´u sau khi x·ª≠ l√Ω:\")\nprint(processed_ds[0][\"text\"][:1000])\n\n# 6Ô∏è‚É£ T√°ch train/test\ntrain_test_split = processed_ds.train_test_split(test_size=0.2, seed=42)\ntrain_dataset = train_test_split[\"train\"]\neval_dataset = train_test_split[\"test\"]\n\nprint(f\"\\nTrain samples: {len(train_dataset)}\")\nprint(f\"Eval samples: {len(eval_dataset)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T09:17:01.469245Z","iopub.execute_input":"2025-11-05T09:17:01.469506Z","iopub.status.idle":"2025-11-05T09:17:34.727870Z","shell.execute_reply.started":"2025-11-05T09:17:01.469487Z","shell.execute_reply":"2025-11-05T09:17:34.727045Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"data.json:   0%|          | 0.00/747M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecc3a25eff7e4dadb7dcdd2876dc3f7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/99086 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4a5b8cdfd944ad089dd1e32290d2694"}},"metadata":{}},{"name":"stdout","text":"üîß ƒêang x·ª≠ l√Ω dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a5e911642b945ebb71128e6491cbd42"}},"metadata":{}},{"name":"stdout","text":"‚úÖ M·∫´u sau khi x·ª≠ l√Ω:\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\nYou are a helpful and empathetic AI assistant supporting users in mental health conversations.<|eot_id|><|start_header_id|>user<|end_header_id|>\nXin ch√†o Alex, t√¥i hy v·ªçng h√¥m nay b·∫°n l√†m t·ªët. T√¥i ch·ªâ mu·ªën n√≥i chuy·ªán v·ªõi ai ƒë√≥ v·ªÅ c·∫£m gi√°c g·∫ßn ƒë√¢y c·ªßa t√¥i. T√¥i c·∫£m th·∫•y th·∫•t v·ªçng v·ªõi s·ª©c kh·ªèe c·ªßa m√¨nh.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nXin ch√†o Charlie! T√¥i ·ªü ƒë√¢y ƒë·ªÉ l·∫Øng nghe v√† h·ªó tr·ª£ b·∫°n. Th·∫≠t tuy·ªát v·ªùi khi b·∫°n ƒë√£ li√™n h·ªá. B·∫°n c√≥ th·ªÉ cho t√¥i bi·∫øt th√™m v·ªÅ nh·ªØng g√¨ ƒëang x·∫£y ra v·ªõi s·ª©c kh·ªèe c·ªßa b·∫°n khi·∫øn b·∫°n th·∫•t v·ªçng kh√¥ng?<|eot_id|><|start_header_id|>user<|end_header_id|>\nC·∫£m ∆°n, Alex. Ch√†, t√¥i ƒëang b·ªã ƒëau ƒë·∫ßu dai d·∫≥ng v√† nƒÉng l∆∞·ª£ng th·∫•p. T√¥i ƒë√£ th·ª≠ c√°c bi·ªán ph√°p kh√°c nhau nh∆∞ng d∆∞·ªùng nh∆∞ kh√¥ng c√≥ t√°c d·ª•ng g√¨. N√≥ kh√° kh√≥ ch·ªãu.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nT√¥i hi·ªÉu b·∫°n c√≥ th·ªÉ b·ª±c b·ªôi ƒë·∫øn m·ª©c n√†o khi c√°c bi·ªán ph√°p ƒëi·ªÅu tr·ªã th√¥ng th∆∞·ªùng c·ªßa b·∫°n kh√¥ng mang l·∫°i hi·ªáu qu·∫£. Nh·ªØng c∆°n ƒëau ƒë\n\nTrain samples: 4000\nEval samples: 1000\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n\nmodel_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    quantization_config=bnb_config,\n    torch_dtype=torch.bfloat16\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\ntokenizer.truncation_side = \"left\"\n\nmodel.config.use_cache = False\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T09:17:34.728726Z","iopub.execute_input":"2025-11-05T09:17:34.729109Z","iopub.status.idle":"2025-11-05T09:20:23.069583Z","shell.execute_reply.started":"2025-11-05T09:17:34.729082Z","shell.execute_reply":"2025-11-05T09:20:23.068697Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aae51f9ba8ef4e16b9da1c5490603609"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cbd80a0a2f54d8399c2cf1b07538cf0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"646283968b8d4e89820c0128cc0a331c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e1f37285df346459ee5d79705b628c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1721332af9e14426bf9349c978a91333"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f24b5faadabd44859a3793af98b268d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d4de2031f764fe198714419377e7b98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad5c4be58f324406b0b39cf432d7f76e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d00287d817d44e5a7f9eec49fc19d3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"821dcf3f988e440a974642da6e077ff8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c85bf19861849b4b2af2f6544fdff0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35c643dc43a147bf8f71ea3b50277a82"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"from peft import LoraConfig\n\nlora_config = LoraConfig(\n    r=8,                          # Rank c·ªßa ma tr·∫≠n low-rank (8 l√† c√¢n b·∫±ng RAM/hi·ªáu qu·∫£)\n    lora_alpha=16,                # H·ªá s·ªë scaling\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0.05,            # Dropout ch·ªëng overfitting\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T09:20:23.071515Z","iopub.execute_input":"2025-11-05T09:20:23.071800Z","iopub.status.idle":"2025-11-05T09:20:23.076123Z","shell.execute_reply.started":"2025-11-05T09:20:23.071781Z","shell.execute_reply":"2025-11-05T09:20:23.075564Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False  # v√¨ ƒë√¢y l√† causal LM\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T09:20:23.076812Z","iopub.execute_input":"2025-11-05T09:20:23.076990Z","iopub.status.idle":"2025-11-05T09:20:23.093856Z","shell.execute_reply.started":"2025-11-05T09:20:23.076975Z","shell.execute_reply":"2025-11-05T09:20:23.093318Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from trl import SFTConfig\n\nsft_config = SFTConfig(\n    output_dir=\"./trained_model\",\n    num_train_epochs=1,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-5,\n    warmup_ratio=0.03,\n    optim=\"paged_adamw_32bit\",\n    lr_scheduler_type=\"constant\",\n    fp16=True,\n    group_by_length=True,\n    save_strategy=\"steps\",\n    logging_steps=50,\n    max_length=1024,\n    report_to=\"none\",\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T09:22:00.842639Z","iopub.execute_input":"2025-11-05T09:22:00.843012Z","iopub.status.idle":"2025-11-05T09:22:00.880043Z","shell.execute_reply.started":"2025-11-05T09:22:00.842990Z","shell.execute_reply":"2025-11-05T09:22:00.879471Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    args=sft_config,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset, \n    data_collator=data_collator,\n    peft_config=lora_config,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T09:22:02.879694Z","iopub.execute_input":"2025-11-05T09:22:02.880425Z","iopub.status.idle":"2025-11-05T09:22:24.094952Z","shell.execute_reply.started":"2025-11-05T09:22:02.880398Z","shell.execute_reply":"2025-11-05T09:22:24.094027Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Adding EOS to train dataset:   0%|          | 0/4000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f73df394282947519a882a4b4a089563"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/4000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acc17d311fb84b68837d1d45844711ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/4000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efc88a2f8e734c55926f2d5fa67240e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding EOS to eval dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13407b3f345e4ff9a8f2b37957b3b94a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing eval dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"565aa383b1cc416fbfbb19aa10dbb896"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating eval dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb43facd1b2541dbb61c07afbf911818"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"trainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T09:22:24.096224Z","iopub.execute_input":"2025-11-05T09:22:24.096514Z","iopub.status.idle":"2025-11-05T15:41:20.251978Z","shell.execute_reply.started":"2025-11-05T09:22:24.096495Z","shell.execute_reply":"2025-11-05T15:41:20.251338Z"}},"outputs":[{"name":"stderr","text":"The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009}.\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 6:18:32, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>1.633000</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.365900</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.272300</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.232200</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.183700</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.144300</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.117400</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.104900</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.086100</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.055400</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>1.059300</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.043800</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>1.035800</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.035900</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>1.034700</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.015200</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>1.019200</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>1.006700</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.991100</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.990100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1000, training_loss=1.1213423881530762, metrics={'train_runtime': 22734.0269, 'train_samples_per_second': 0.176, 'train_steps_per_second': 0.044, 'total_flos': 1.818527602768036e+17, 'train_loss': 1.1213423881530762, 'epoch': 1.0})"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"trainer.push_to_hub(\"Upload LoRA fine-tuned adapter\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T15:41:20.252762Z","iopub.execute_input":"2025-11-05T15:41:20.253006Z","iopub.status.idle":"2025-11-05T15:41:24.977179Z","shell.execute_reply.started":"2025-11-05T15:41:20.252978Z","shell.execute_reply":"2025-11-05T15:41:24.976526Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"757abc0f2e4749bba81bf3008826359a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05b104293d7a45f3b1ed9433bb3b750c"}},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/letri345/trained_model/commit/4d419b6b78b6512552f171d25d44dcd57753fe48', commit_message='Upload LoRA fine-tuned adapter', commit_description='', oid='4d419b6b78b6512552f171d25d44dcd57753fe48', pr_url=None, repo_url=RepoUrl('https://huggingface.co/letri345/trained_model', endpoint='https://huggingface.co', repo_type='model', repo_id='letri345/trained_model'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}